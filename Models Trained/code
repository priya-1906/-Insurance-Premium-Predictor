from sklearn.linear_model import LinearRegression

linreg = LinearRegression()
linreg.fit(X_train_pp, y_train)

pred_val_lr = linreg.predict(X_val_pp)
report_metrics(y_val, pred_val_lr, label="[LinearRegression] ")
------------------------------------------------------------------------
from sklearn.compose import TransformedTargetRegressor
from sklearn.preprocessing import FunctionTransformer

# y ↦ log1p(y) during fit, invert with expm1 at predict
log_ttr = TransformedTargetRegressor(
    regressor=LinearRegression(),
    func=np.log1p,
    inverse_func=np.expm1
)
log_ttr.fit(X_train_pp, y_train)

pred_val_lrlog = log_ttr.predict(X_val_pp)
report_metrics(y_val, pred_val_lrlog, label="[LinReg + log1p(y)] ")
-------------------------------------------------------------------------
from sklearn.ensemble import RandomForestRegressor

# Full Random Forest configuration
rf_full = RandomForestRegressor(
    n_estimators=200,     # 200 trees (good balance; can increase to 300–400 if time permits)
    max_depth=25,         # prevents trees from growing too deep (faster, avoids overfit)
    min_samples_split=4,  # split control
    min_samples_leaf=2,   # leaf control
    n_jobs=-1,            # use all CPU cores
    random_state=42,
    verbose=1             # progress updates
)

# Train on full training data
rf_full.fit(X_train_pp, y_train)

# Predict on validation
pred_val_rf_full = rf_full.predict(X_val_pp)

# Report metrics
report_metrics(y_val, pred_val_rf_full, label="[RandomForest FULL DATA] ")
--------------------------------------------------------------------------------------
from xgboost import XGBRegressor

xgb = XGBRegressor(
    n_estimators=600,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    random_state=42,
    n_jobs=-1,
    tree_method="hist"  # fast on large data
)
xgb.fit(X_train_pp, y_train)

pred_val_xgb = xgb.predict(X_val_pp)
report_metrics(y_val, pred_val_xgb, label="[XGBRegressor] ")
-----------------------------------------------------------------------------------
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor

# --------------------------
# 1) Parameter search space
# --------------------------
param_dist = {
    "n_estimators": [400, 600, 800],
    "max_depth": [6, 8, 10],
    "learning_rate": [0.03, 0.05, 0.08],
    "subsample": [0.7, 0.8, 1.0],
    "colsample_bytree": [0.7, 0.8, 1.0],
    "reg_lambda": [0.5, 1.0, 2.0]
}

xgb_base = XGBRegressor(
    random_state=42,
    n_jobs=-1,
    tree_method="hist"   # fast histogram algorithm
)

# --------------------------
# 2) Quick tuning on subset
# --------------------------
# For speed, use only 200k rows (adjust if memory/CPU is stronger)
X_tune = X_train_pp[:200000]
y_tune = y_train[:200000]

rand = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_dist,
    n_iter=5,   # quick test (try only 5 combos instead of 20)
    scoring="neg_root_mean_squared_error",  # optimize RMSE
    cv=3,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

rand.fit(X_tune, y_tune)

best_xgb = rand.best_estimator_
print("Best params (subset run):", rand.best_params_)

# --------------------------
# 3) Evaluate on validation set
# --------------------------
pred_val_best = best_xgb.predict(X_val_pp)
report_metrics(y_val, pred_val_best, label="[XGB Tuned] ")

# --------------------------
# 4) (Optional) Retrain on full train set with best params
# --------------------------
best_xgb_full = XGBRegressor(
    **rand.best_params_,
    random_state=42,
    n_jobs=-1,
    tree_method="hist"
)
best_xgb_full.fit(X_train_pp, y_train)

pred_val_full = best_xgb_full.predict(X_val_pp)
report_metrics(y_val, pred_val_full, label="[XGB Tuned FULL DATA] ")
--------------------------------------------------------------------
# Choose the winner:
best_model = best_xgb  # or rf / log_ttr / xgb

# Fit on all training data
X_all_pp = preprocessor.fit_transform(X_full)  # refit preprocessor on all train features
best_model.fit(X_all_pp, y_full)

# Transform test and predict
X_test_pp = preprocessor.transform(X_test)
test_preds = best_model.predict(X_test_pp)
-------------------------------------------------------------------
import numpy as np
from sklearn.compose import TransformedTargetRegressor
from xgboost import XGBRegressor, callback as xgb_callback

# 1) Build the base regressor: put eval_metric in the constructor (version-safe)
xgb_log = XGBRegressor(
    n_estimators=3000,      # high cap; ES will pick best
    max_depth=8,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=0.5,
    random_state=42,
    n_jobs=-1,
    tree_method="hist",
    eval_metric="rmse"      # <-- set here, not in fit()
)

# 2) Wrap with log-target
ttr_xgb = TransformedTargetRegressor(
    regressor=xgb_log,
    func=np.log1p,
    inverse_func=np.expm1
)

# 3) Early stopping via callbacks (most compatible)
es_cb = xgb_callback.EarlyStopping(rounds=100, save_best=True)

# 4) Fit with ES; if callbacks not supported in your build, fall back to no-ES
try:
    ttr_xgb.fit(
        X_train_pp, y_train,
        eval_set=[(X_val_pp, y_val)],
        callbacks=[es_cb]   # version-safe ES
        # NOTE: don't pass eval_metric or early_stopping_rounds to fit()
    )
except TypeError:
    # Fallback: train without early stopping (still valid)
    ttr_xgb.fit(X_train_pp, y_train)

# 5) Predict & report
pred_val_xgb_log = ttr_xgb.predict(X_val_pp)
report_metrics(y_val, pred_val_xgb_log, label="[XGB + log1p(y)] ")

# (Optional) print trees used if ES worked
best_rounds = getattr(ttr_xgb.regressor_, "best_iteration", None)
if best_rounds is not None:
    print("Best trees used:", best_rounds + 1)
------------------------------------------------------------------------
  from xgboost import XGBRegressor, callback as xgb_callback

# Put eval_metric in constructor (works across versions)
xgb_es = XGBRegressor(
    n_estimators=4000,
    max_depth=8,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=0.5,
    random_state=42,
    n_jobs=-1,
    tree_method="hist",
    eval_metric="rmse"   # <= set here
)

# Early stopping via callbacks (version-safe)
es_cb = xgb_callback.EarlyStopping(rounds=100, save_best=True)

# Fit with ES; if your build still rejects callbacks, fall back to no-ES
try:
    xgb_es.fit(
        X_train_pp, y_train,
        eval_set=[(X_val_pp, y_val)],
        callbacks=[es_cb]   # <= no early_stopping_rounds in fit
    )
except TypeError:
    xgb_es.fit(X_train_pp, y_train)  # fallback

# Predict & report
pred_val_es = xgb_es.predict(X_val_pp)
report_metrics(y_val, pred_val_es, label="[XGB EarlyStop] ")

# Best trees used (if ES worked)
best_iter = getattr(xgb_es, "best_iteration", None)
if best_iter is not None:
    print("Best trees used:", best_iter + 1)
